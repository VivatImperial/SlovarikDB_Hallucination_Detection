# :trophy: X5 Tech Hack - алгоритм для детектировании аномалий в LLM
## ❓ Задача
Нам необходимо было разработать модель, которая будет классифировать тексты на две категории: корректный текст / текст с содержанием “галлюцинаций”. Это позволит избежать случаев дезинформации пользователя или предоставления ему некорректных результатов. Проблемой стали некоторые темы: город, года - которые мы обрабатываем специальным промтом и LLM.
### :tada: Результат
#### :trophy: Место: **5/40 место**</br>
**Score:** </br>
F1 Micro: **0.93**</br>
### :page_facing_up: Данные
Данные представляют собой набор [текст документа] - [контекст] - [бинарная метка] (в обучающем множестве). Для тестового множества требуется по тексту документа и контексту определить метку документа. Из-за малого кол-ва данных мы сгененрировали синтетику благодрая Mixtral 8x22B, которая принесла + 0.02 F1 score.
### :memo: Решение: №1
В качестве классификатора мы используем  microsoft/deberta-v3-base  дообученный на галлюцинациях при суммаризации на английском языке. 
**Базовая модель**: vectara/hallucination_evaluation_model. Дообучалась на train выборке.
F1-score - 0.92
### :memo: Решение: №2
Было замечено, что ошибки bert-like решения типовые и тематические и связаны с факт-чекингом. Для валидации фактов можно использовать RAG-подход: LLM и “контекст” вопроса. <br>
![image](https://github.com/daniil-dushenev/gagarin-hack/assets/44606552/5d71a43e-8805-4f61-bb37-a947b5c0916c) <br>
ОДНАКО: из-за серверных проблем пайплайн не запускался. Score не был обсчитан. 
ЛОКАЛЬНО: увеличение F1_score с 0.92 до 0.97. <br>
**Алгоритм**: 1) Проверка длинны ответа модели 2) Поиск дат в ответе модели c помощью regex 3) Поиск названий городов, стран в ответе модели благодаря NER через SlovNet 4.1) Передает в LLM вопросы с датами или NERами для фактчекинга, в зависимости от характера изменяется промт. 4.2) Передаем в DeBerta остальные вопросы длы быстрого ответа.
### :memo: LLM
**SOTA-решения:** использование LLM и банка знаний для фактчекинга; <br>
**Наше решение:** saiga_llama3_8b_q4 с настраиваемым промтом; <br>
**Промт:** проверка пары вопрос-ответ по контексту с уточнением “что именно” проверять; <br>
**Стек:** llama.cpp <br>
### :bulb: Уникальность и Точки роста
1) Гибкость настройки 2) Простота масштабирования 3) Высокая точность: f1 score: 0.97 4) Универсальность: Пайплайн позволяет автоматически обрабатывать аномалии разных типов разными подходами.<br>
1) Интеграция мощных LLM 2) Подключение “банка знаний” 3) Собственные NERы 4) Создание RESTful-API
